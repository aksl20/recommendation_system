{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Context__\n",
    "\n",
    "The goal of this notebook is to build a recommendation system based on the [Entree Chicago Recommendation](http://archive.ics.uci.edu/ml/datasets/Entree+Chicago+Recommendation+Data) dataset with the distributed algorithm system [Spark](https://spark.apache.org/). By building this recommendation system, we will cover several methods used for natural language processing and recommendation system. \n",
    "- Content-base recommendation\n",
    "- Collaboration filtering\n",
    "- Tf IDF algorithm\n",
    "- Hybrid filtering\n",
    "\n",
    "__Dataset Information__\n",
    "\n",
    "The dataset contained two subdirectories. One contains the data about users' sessions and the other the restaurants' informations. Each subfolder contains a readme file with more details about the dataset.\n",
    "\n",
    "__1.__ Users' Sessions\n",
    "\n",
    "This data records interactions with Entree Chicago restaurant recommendation system (originally [Web Link](http://infolab.cs.uchicago.edu/entree)) from September, 1996 to April, 1999. The data is organized into files roughly spanning a quarter year -- with Q3 1996 and Q2 1999 each only containing one month.\n",
    "\n",
    "Each line in a session file represents a session of user interaction with the system. The (tab-separated) fields are as follows:\n",
    "\n",
    "- date: Date of the connection\n",
    "- ip: ip adress of the user\n",
    "- entry point: Users can use a restaurant from any city as a entry point, but they always get recommendations for Chicago restaurants. Entry points have the form nnnX, where nnn is a numeric restaurant ID and X is a character A-H that encodes the city.\n",
    "- rates: These are all Chicago restaurants. These entries have the form nnnX, where nnn is a numeric restaurant ID and X is a character L-T that encodes the navigation operation (see readme for more details).\n",
    "- end point: Just the numeric id for the (Chicago) restaurant that the user saw last\n",
    "\n",
    "Bellow an example of the data\n",
    "\n",
    "|        date        |           ip       | entry point |     rates    | end point\n",
    "-------------------- | ------------------ | ----------- | ------------ | ---------\n",
    "01/Oct/1996:10:08:41 | keeper.tribune.com |      0      | 369N    369P |    -1\n",
    "01/Oct/1996:11:34:23 | 128.103.79.152     |      0      | 387L    245L |    245\n",
    "\n",
    "__2.__ Restaurant information\n",
    "\n",
    "In addition to the user's interactions, there is also data linking the restaurant ID with its name and features such as \"fabulous wine lists\", \"good for younger kids\", and \"Ethopian\" cuisine. This data is stored by city (e.g. Atlanta, Boston, etc.) and is in the following format:\n",
    "\n",
    "- id: ID of the restaurant\n",
    "- name: name of the restaurant\n",
    "- description: features about the restaurant\n",
    "\n",
    "Bellow an example of the data\n",
    "\n",
    "|        id        |           name       |             description                             |\n",
    "-------------------| -------------------- | --------------------------------------------------- |\n",
    "0000436            | La Fontanella        | 214 249 229 125 075 205 052 165                     |\n",
    "0000437            | Retro Bistro         | 137 174 200 196 193 191 192 025 092 076 206 053 166 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.ml.feature as proc\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "project_dir = Path.cwd().parents[1]\n",
    "data_dir = project_dir / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-01 12:47:10--  http://archive.ics.uci.edu/ml/machine-learning-databases/entree-mld/entree_data.tar.gz\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1074349 (1.0M) [application/x-httpd-php]\n",
      "Saving to: ‘entree_data.tar.gz’\n",
      "\n",
      "entree_data.tar.gz  100%[===================>]   1.02M   527KB/s    in 2.0s    \n",
      "\n",
      "2019-12-01 12:47:13 (527 KB/s) - ‘entree_data.tar.gz’ saved [1074349/1074349]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/entree-mld/entree_data.tar.gz'\n",
    "!cd $data_dir && wget $url && tar -xzf entree_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content based filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use content-based filtering to provided the recommendation for the user. By using the restaurant's features provided in the dataset, we can compute the tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mapping for tokens to words\n",
    "tokens_to_words = dict(sc.textFile(str(data_dir / 'entree/data/features.txt'))\\\n",
    "                    .map(lambda line: line.split('\\t'))\n",
    "                    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_restaurants_data(file):\n",
    "    raw_data = sc.textFile(file)\\\n",
    "                 .map(lambda line: line.split('\\t'))\\\n",
    "                 .map(lambda column: (column[0], column[1], ' '.join(column[2].split(' ')[:-1]), column[2].split(' ')[-1]))\n",
    "    return raw_data.toDF(('id', 'name', 'description', 'price'))\n",
    "    \n",
    "df = transform_restaurants_data(str(data_dir / \"entree/data/chicago.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-----+\n",
      "|     id|                name|         description|price|\n",
      "+-------+--------------------+--------------------+-----+\n",
      "|0000000|          Moti Mahal|214 035 149 021 1...|  163|\n",
      "|0000001|             Village|026 249 174 004 1...|  165|\n",
      "|0000002|Millrose Brewing ...|137 249 194 215 2...|  165|\n",
      "|0000003|       Dover Straits|137 190 174 249 2...|  165|\n",
      "|0000004| Eat Your Hearts Out|214 249 249 197 1...|  164|\n",
      "|0000005|  Pizzeria Uno & Due|026 249 004 132 1...|  163|\n",
      "|0000006|    Trattoria Franco|214 136 125 078 2...|  170|\n",
      "|0000007|    Little Bucharest|214 004 132 249 1...|  164|\n",
      "|0000008|             Pattaya| 026 235 074 205 051|  163|\n",
      "|0000009|      House of Hunan|026 191 192 024 0...|  164|\n",
      "|0000010| Morton's of Chicago|137 174 099 249 2...|  168|\n",
      "|0000011|             Jezebel|214 174 249 200 1...|  166|\n",
      "|0000012|               Capri|137 249 174 249 1...|  164|\n",
      "|0000013|          Don Roth's|137 174 249 063 2...|  166|\n",
      "|0000014|       Tucci Benucch|026 249 191 192 1...|  165|\n",
      "|0000015|           Rim Klong| 214 235 075 205 052|  163|\n",
      "|0000016|Four Farthings Ta...|214 249 194 215 2...|  164|\n",
      "|0000017|  Skip's Other Place|137 174 100 249 1...|  164|\n",
      "|0000018|         Papa Milano|026 249 198 125 0...|  164|\n",
      "|0000019| Gladys Luncheonette|214 249 174 148 2...|  163|\n",
      "+-------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tf-IDF__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedings given for the restaurant data are not usable by spark. So, we transform the features into sentence and aply tf-if with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = proc.Tokenizer(inputCol=\"description\", outputCol=\"tokens\")\n",
    "tokens_df = tokenizer.transform(df).rdd.map(lambda row: (row.id, row.name, row.description, row.price, \n",
    "                                           [tokens_to_words[token].split(' ') for token in row.tokens]))\\\n",
    "                                            .toDF(('id', 'name', 'description', 'price', 'tokens'))\n",
    "tokens_df = tokens_df.withColumn('tokens', F.flatten(tokens_df['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = proc.CountVectorizer(inputCol='tokens', outputCol='tf')\\\n",
    "                       .fit(tokens_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cosine similarity__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common distance metric is cosine similarity. The metric can be thought of geometrically if one treats a given item’s row of the ratings matrix as a vector. For content-based filtering, two item similarity is measured as the cosine of the angle between the two items’ vectors. The class below compute a matrix cosinus similarity for each restaurant in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosinusSimilarityClassifier:\n",
    "    \"\"\"Class to compute matrix cosinus similarity based on a column description\n",
    "    from the input dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.normalizer = None\n",
    "        self.cosine_similarity = None\n",
    "        self.count_vectorizer = None\n",
    "        self.idfModel = None\n",
    "    \n",
    "    def _fit_tf_idf(self, df, tokens_col, output_col):\n",
    "        self.count_vectorizer = proc.CountVectorizer(inputCol=tokens_col, outputCol='tf').fit(df)\n",
    "        tf_df = self.count_vectorizer.transform(tokens_df)\n",
    "        \n",
    "        self.idfModel = proc.IDF(inputCol=\"tf\", outputCol=\"idf\").fit(tf_df)\n",
    "        idf_df = self.idfModel.transform(tf_df)\n",
    "        \n",
    "        self.normalizer = proc.Normalizer(inputCol='idf', outputCol=output_col)\n",
    "        return self.normalizer.transform(idf_df)\n",
    "    \n",
    "    def _fit_matrix_similarity(self, df, id_col, embedding_col):\n",
    "        mat = IndexedRowMatrix(df.select(id_col, embedding_col)\\\n",
    "                                 .rdd.map(lambda row: IndexedRow(row.id, row.norm.toArray())))\\\n",
    "                                 .toBlockMatrix()\n",
    "        dot = mat.multiply(mat.transpose())\n",
    "        self.cosine_similarity = dot.toLocalMatrix().toArray()\n",
    "        return self\n",
    "    \n",
    "    def fit(self, df, tokens_col, id_col):\n",
    "        df = self._fit_tf_idf(df, tokens_col=tokens_col, output_col='norm')\n",
    "        self._fit_matrix_similarity(df, id_col=id_col, embedding_col='norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_classifier = CosinusSimilarityClassifier()\n",
    "cos_sim_classifier.fit(df=tokens_df, tokens_col='tokens', id_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test with users__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a user which like restaurant as the first one of the dataset (Moti Mahal) which serve indian food, we can test the cosinus similarity method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moti Mahal 0.9999999999999999\n",
      "Standard India 0.9489745950547768\n",
      "Udupi Palace 0.8097034711916234\n",
      "Natraj 0.8073763310007285\n",
      "Shree 0.7855653336442743\n",
      "Gandhi Indian 0.6909826463283292\n",
      "Mei Shung 0.6519763691217288\n",
      "Formosa 0.6495665383643229\n",
      "Anna Maria Pasteria 0.6437300279238267\n",
      "Pine Yard 0.6422507002813812\n"
     ]
    }
   ],
   "source": [
    "# List the k histest similarities obtained for the first restaurant of the matrix\n",
    "k = 10\n",
    "similarities = np.sort(cos_sim_classifier.cosine_similarity[0])[::-1][:k]\n",
    "restaurants = [df.select('name').where('id = {}'.format(restaurant)).collect()[0][0] \n",
    "               for restaurant in cos_sim_classifier.cosine_similarity[0].argsort()[::-1][:k]]\n",
    "\n",
    "for i in range(10):\n",
    "    print(restaurants[i], similarities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_session_data(file):\n",
    "    raw_data = sc.textFile(file)\\\n",
    "                 .map(lambda line: line.split('\\t', 3))\\\n",
    "                 .map(lambda features: (features[0], features[1], features[2], \n",
    "                                        ' '.join(features[3].split('\\t')[:-1]), features[3].split('\\t')[-1]))\n",
    "    return raw_data.toDF(('date', 'ip', 'entry_point', 'rates', 'end_point'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_session = transform_session_data(str(data_dir / 'entree/session/session.1996-Q4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------------+---------+\n",
      "|                date|                  ip|entry_point|               rates|end_point|\n",
      "+--------------------+--------------------+-----------+--------------------+---------+\n",
      "|01/Oct/1996:10:08...|  keeper.tribune.com|          0|           369N 369P|       -1|\n",
      "|01/Oct/1996:11:32...|        mail.smc.com|          0|                204L|      505|\n",
      "|01/Oct/1996:11:34...|      128.103.79.152|          0|           387L 245L|      245|\n",
      "|01/Oct/1996:11:33...|        mail.smc.com|          0|                369N|       -1|\n",
      "|01/Oct/1996:11:35...|        mail.smc.com|          0|                465L|      438|\n",
      "|25/Sep/1996:19:53...|www-r5.proxy.aol.com|          0|       19L 558N 543N|      192|\n",
      "|01/Oct/1996:11:49...|       proxy.hud.gov|          0|            19L 558L|      558|\n",
      "|01/Oct/1996:12:04...|www-r5.proxy.aol.com|          0|      598L 483L 421L|      598|\n",
      "|26/Sep/1996:21:18...|www-q5.proxy.aol.com|          0|274Q 122Q 369L 56...|      192|\n",
      "|01/Oct/1996:14:57...|orange.lawbulleti...|          0|                369N|       -1|\n",
      "|01/Oct/1996:14:58...|orange.lawbulleti...|          0|                369N|       -1|\n",
      "|01/Oct/1996:16:46...|      205.177.86.147|          0|      369L 303L 111L|      632|\n",
      "|01/Oct/1996:17:00...|      205.177.86.147|          0|62L 536L 379L 610...|      226|\n",
      "|27/Sep/1996:17:03...|     cosmo.sabre.net|          0|            387N 14N|      230|\n",
      "|01/Oct/1996:21:19...|      207.146.84.176|          0|                369N|       -1|\n",
      "|01/Oct/1996:21:22...|      207.146.84.176|          0|   652L 110N 17L 53L|      560|\n",
      "|01/Oct/1996:21:23...|      207.146.84.176|          0|                452N|      334|\n",
      "|02/Oct/1996:08:24...|tyrannosaurus.csd...|          0|                379P|      144|\n",
      "|02/Oct/1996:08:40...|tyrannosaurus.csd...|          0|                459L|       87|\n",
      "|02/Oct/1996:08:44...|tyrannosaurus.csd...|        87C|           122S 627Q|       -1|\n",
      "+--------------------+--------------------+-----------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get taste of users__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second file show us the ratings given by the users. each user that navigate on the website can rates on a particular restaurant by giving a letter. There is different type of evaluation depending on the letter (see readme of the data for more details). By extract each letter used by the user, it's possible to obtain some information about his tastes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_to_personalities = dict(M='spendthrift', P='traditional', Q='creative', R='fun', S='quiet')\n",
    "\n",
    "def get_personalities(rates, mapping):\n",
    "    letters = ''.join(filter(str.isalpha, rates))\n",
    "    personalities = ' '.join(set([mapping.get(letter) for letter in letters if mapping.get(letter)]))\n",
    "    return personalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_session.rdd.map(lambda row: (*row, get_personalities(row.rates, mapping=letters_to_personalities)))\\\n",
    "              .toDF(('date', 'ip', 'entry_point', 'rates', 'end_point', 'personality'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(df, col_name):\n",
    "    col_attributes = set(df.select(col_name).distinct().rdd\\\n",
    "                                        .map(lambda row: row.personality.split(' '))\\\n",
    "                                        .flatMap(lambda x: x)\\\n",
    "                                        .collect())\n",
    "    col_expr = [F.when(F.col(col_name) == ty, 1).otherwise(0).alias(ty) for ty in col_attributes]\n",
    "    return df.select(\"*\", *col_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['date', 'ip'] + list(letters_to_personalities.values()) + ['rates']\n",
    "users_rates = get_dummies(df, 'personality').select(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+-----------+--------+---+-----+--------------------+\n",
      "|                date|                  ip|spendthrift|traditional|creative|fun|quiet|               rates|\n",
      "+--------------------+--------------------+-----------+-----------+--------+---+-----+--------------------+\n",
      "|01/Oct/1996:10:08...|  keeper.tribune.com|          0|          1|       0|  0|    0|           369N 369P|\n",
      "|01/Oct/1996:11:32...|        mail.smc.com|          0|          0|       0|  0|    0|                204L|\n",
      "|01/Oct/1996:11:34...|      128.103.79.152|          0|          0|       0|  0|    0|           387L 245L|\n",
      "|01/Oct/1996:11:33...|        mail.smc.com|          0|          0|       0|  0|    0|                369N|\n",
      "|01/Oct/1996:11:35...|        mail.smc.com|          0|          0|       0|  0|    0|                465L|\n",
      "|25/Sep/1996:19:53...|www-r5.proxy.aol.com|          0|          0|       0|  0|    0|       19L 558N 543N|\n",
      "|01/Oct/1996:11:49...|       proxy.hud.gov|          0|          0|       0|  0|    0|            19L 558L|\n",
      "|01/Oct/1996:12:04...|www-r5.proxy.aol.com|          0|          0|       0|  0|    0|      598L 483L 421L|\n",
      "|26/Sep/1996:21:18...|www-q5.proxy.aol.com|          0|          0|       1|  0|    0|274Q 122Q 369L 56...|\n",
      "|01/Oct/1996:14:57...|orange.lawbulleti...|          0|          0|       0|  0|    0|                369N|\n",
      "|01/Oct/1996:14:58...|orange.lawbulleti...|          0|          0|       0|  0|    0|                369N|\n",
      "|01/Oct/1996:16:46...|      205.177.86.147|          0|          0|       0|  0|    0|      369L 303L 111L|\n",
      "|01/Oct/1996:17:00...|      205.177.86.147|          0|          0|       0|  0|    0|62L 536L 379L 610...|\n",
      "|27/Sep/1996:17:03...|     cosmo.sabre.net|          0|          0|       0|  0|    0|            387N 14N|\n",
      "|01/Oct/1996:21:19...|      207.146.84.176|          0|          0|       0|  0|    0|                369N|\n",
      "|01/Oct/1996:21:22...|      207.146.84.176|          0|          0|       0|  0|    0|   652L 110N 17L 53L|\n",
      "|01/Oct/1996:21:23...|      207.146.84.176|          0|          0|       0|  0|    0|                452N|\n",
      "|02/Oct/1996:08:24...|tyrannosaurus.csd...|          0|          1|       0|  0|    0|                379P|\n",
      "|02/Oct/1996:08:40...|tyrannosaurus.csd...|          0|          0|       0|  0|    0|                459L|\n",
      "|02/Oct/1996:08:44...|tyrannosaurus.csd...|          0|          0|       0|  0|    0|           122S 627Q|\n",
      "+--------------------+--------------------+-----------+-----------+--------+---+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_rates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our categoricals features are not very usefull to apply collaborative filtering. We need more information about the users. However, it's possible to weight the prediction made with the cosinus similatiry by using the information about \"this restaurant is quiet or fun ?\". By doing that, we will able to give a better recommendation based on the restaurant description and the taste of the user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
